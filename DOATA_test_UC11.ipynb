{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "history_visible": true,
      "mount_file_id": "1CoAdhuCC-EzgVuC0rV1DsP0M7J-uZ2fE",
      "authorship_tag": "ABX9TyMcu/X6D9a9cWdAkPjv6BP+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NajibaTagougui/HPE/blob/main/DOATA_test_UC11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oD6p2tNYLNEo",
        "outputId": "538f6eaa-a70a-4543-deb4-64f8f60c6301"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-14c93b4cf2a5>:361: UserWarning: No HF_TOKEN found, using anonymous access\n",
            "  warnings.warn(\"No HF_TOKEN found, using anonymous access\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1283 videos for train split\n",
            "Found 328 videos for test split\n",
            "\n",
            "Training samples: 1283\n",
            "Test samples: 328\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTModel: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.io import read_video\n",
        "from torchvision.transforms import Compose, Resize, Normalize\n",
        "from transformers import ViTModel\n",
        "import cv2\n",
        "import warnings\n",
        "import numpy as np\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple\n",
        "\n",
        "# Configuration\n",
        "@dataclass\n",
        "class Config:\n",
        "    # Dataset\n",
        "    root_dir: str = \"/content/drive/MyDrive/UCF11\"  # Changed to UCF11\n",
        "    frame_size: int = 224\n",
        "    num_frames: int = 32\n",
        "    batch_size: int = 8\n",
        "\n",
        "    # Model\n",
        "    embed_dim: int = 768\n",
        "    num_heads: int = 8\n",
        "    num_layers: int = 4\n",
        "\n",
        "    # Training\n",
        "    lr: float = 1e-4\n",
        "    epochs: int = 50\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # Occlusion\n",
        "    occlusion_radius: int = 25\n",
        "    occlusion_speed: float = 2.0\n",
        "    occlusion_enabled: bool = True\n",
        "\n",
        "# Dataset\n",
        "class UCF11Dataset(Dataset):\n",
        "    def __init__(self, file_list: List[Tuple[str, int]], transform=None, config=None):\n",
        "        self.file_list = file_list\n",
        "        self.transform = transform or Compose([\n",
        "            Resize((config.frame_size, config.frame_size)),\n",
        "            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        self.config = config\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.file_list[idx]\n",
        "        try:\n",
        "            video, mask = self._read_video(path)\n",
        "            video = self.transform(video)\n",
        "            return {\n",
        "                'video': video.float(),  # Ensure float type\n",
        "                'mask': mask.float(),    # Ensure float type\n",
        "                'label': torch.tensor(label, dtype=torch.long)  # Ensure long type\n",
        "            }\n",
        "        except Exception as e:\n",
        "            warnings.warn(f\"Error loading video {path}: {str(e)}\")\n",
        "            # Return empty tensors of correct shape\n",
        "            dummy_video = torch.zeros((self.config.num_frames, 3, self.config.frame_size, self.config.frame_size))\n",
        "            dummy_mask = torch.zeros((self.config.num_frames, 1, self.config.frame_size, self.config.frame_size))\n",
        "            return {\n",
        "                'video': dummy_video,\n",
        "                'mask': dummy_mask,\n",
        "                'label': torch.tensor(-1, dtype=torch.long)  # Invalid label\n",
        "            }\n",
        "\n",
        "    def _read_video(self, path):\n",
        "        try:\n",
        "            # Try PyAV first\n",
        "            video, _, _ = read_video(path, pts_unit='sec')\n",
        "            video = video.permute(0, 3, 1, 2).float() / 255.0\n",
        "        except Exception as e:\n",
        "            # Fallback to OpenCV\n",
        "            warnings.warn(f\"PyAV failed for {path}: {e}, using OpenCV\")\n",
        "            video = self._read_with_opencv(path)\n",
        "\n",
        "        # Ensure we have the correct number of frames\n",
        "        if len(video) > self.config.num_frames:\n",
        "            indices = torch.linspace(0, len(video)-1, self.config.num_frames).long()\n",
        "            video = video[indices]\n",
        "        elif len(video) < self.config.num_frames:\n",
        "            # Pad with last frame if needed\n",
        "            last_frame = video[-1].unsqueeze(0)\n",
        "            padding = last_frame.repeat(self.config.num_frames - len(video), 1, 1, 1)\n",
        "            video = torch.cat([video, padding], dim=0)\n",
        "\n",
        "        # Generate occlusion mask\n",
        "        mask = self._generate_occlusion(video)\n",
        "        return video, mask\n",
        "\n",
        "    def _read_with_opencv(self, path):\n",
        "        cap = cv2.VideoCapture(str(path))\n",
        "        frames = []\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret: break\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frames.append(torch.from_numpy(frame))\n",
        "        cap.release()\n",
        "        if not frames:\n",
        "            return torch.zeros((self.config.num_frames, 3, 240, 320))  # Default resolution if empty\n",
        "        video = torch.stack(frames).permute(0, 3, 1, 2).float() / 255.0\n",
        "        return video\n",
        "\n",
        "    def _generate_occlusion(self, video):\n",
        "        T, C, H, W = video.shape\n",
        "        mask = torch.zeros(T, 1, H, W)\n",
        "\n",
        "        if self.config.occlusion_enabled:\n",
        "            x = torch.randint(0, W, (1,))\n",
        "            y = torch.randint(0, H, (1,))\n",
        "            for t in range(T):\n",
        "                x = x + self.config.occlusion_speed\n",
        "                y = y + torch.randn(1) * 0.5\n",
        "                radius = self.config.occlusion_radius\n",
        "\n",
        "                Y, X = torch.meshgrid(torch.arange(H), torch.arange(W))\n",
        "                dist = ((X - x)/radius)**2 + ((Y - y)/radius)**2\n",
        "                mask[t] = torch.exp(-dist)\n",
        "        return mask\n",
        "\n",
        "\n",
        "\n",
        "# Model Components\n",
        "class OcclusionModulatedAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.embed_dim = config.embed_dim\n",
        "        self.num_heads = config.num_heads\n",
        "        self.head_dim = self.embed_dim // self.num_heads\n",
        "\n",
        "        self.qkv = nn.Linear(self.embed_dim, 3 * self.embed_dim)\n",
        "        self.proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "        self.alpha = nn.Parameter(torch.tensor(0.5))\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        B, T, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, T, N, 3, self.num_heads, self.head_dim).permute(3, 0, 1, 4, 2, 5)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * (self.head_dim ** -0.5)\n",
        "        mask = mask.view(B, T, N).unsqueeze(2).unsqueeze(3)\n",
        "        attn = attn * (self.alpha * mask + (1 - self.alpha))\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "\n",
        "        x = (attn @ v).transpose(2, 3).reshape(B, T, N, C)\n",
        "        return self.proj(x)\n",
        "\n",
        "class OcclusionEstimator(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.local_cnn = nn.Sequential(\n",
        "            nn.Conv2d(config.embed_dim, config.embed_dim, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(config.embed_dim, config.embed_dim, kernel_size=3, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=config.embed_dim,\n",
        "            nhead=config.num_heads,\n",
        "            dim_feedforward=config.embed_dim*4\n",
        "        )\n",
        "        self.global_transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
        "\n",
        "        self.pred_head = nn.Sequential(\n",
        "            nn.Conv2d(config.embed_dim, 64, kernel_size=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 1, kernel_size=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, N, C = x.shape\n",
        "        P = int(N ** 0.5)\n",
        "        x = x.permute(0, 1, 3, 2).reshape(B*T, C, P, P)\n",
        "\n",
        "        local_feats = self.local_cnn(x)\n",
        "        global_feats = local_feats.flatten(2).permute(2, 0, 1)\n",
        "        global_feats = self.global_transformer(global_feats)\n",
        "        global_feats = global_feats.permute(1, 2, 0).reshape(B*T, C, P, P)\n",
        "\n",
        "        combined = local_feats + global_feats\n",
        "        occlusion_maps = self.pred_head(combined)\n",
        "        occlusion_maps = F.interpolate(\n",
        "            occlusion_maps,\n",
        "            size=(self.config.frame_size, self.config.frame_size),\n",
        "            mode='bilinear',\n",
        "            align_corners=False\n",
        "        )\n",
        "        return occlusion_maps.reshape(B, T, self.config.frame_size, self.config.frame_size)\n",
        "\n",
        "class TemporalAggregator(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([TemporalBlock(config) for _ in range(config.num_layers)])\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, config.num_frames, 1, config.embed_dim) * 0.02)\n",
        "        self.norm = nn.LayerNorm(config.embed_dim)\n",
        "\n",
        "    def forward(self, x, occlusion_maps):\n",
        "        B, T, N, C = x.shape\n",
        "        H = W = int(N ** 0.5)\n",
        "\n",
        "        x = x + self.pos_embed[:, :T]\n",
        "        attn_masks = F.interpolate(\n",
        "            occlusion_maps.flatten(0, 1).unsqueeze(1),\n",
        "            size=(H, W),\n",
        "            mode='bilinear',\n",
        "            align_corners=False\n",
        "        ).view(B, T, H * W)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, attn_masks)\n",
        "        return self.norm(x)\n",
        "\n",
        "class TemporalBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.attention = OcclusionModulatedAttention(config)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(config.embed_dim, config.embed_dim * 4),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(config.embed_dim * 4, config.embed_dim)\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(config.embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(config.embed_dim)\n",
        "\n",
        "    def forward(self, x, attn_masks):\n",
        "        attn_out = self.attention(self.norm1(x), attn_masks)\n",
        "        x = x + attn_out\n",
        "        mlp_out = self.mlp(self.norm2(x))\n",
        "        x = x + mlp_out\n",
        "        return x\n",
        "\n",
        "# Main DOATA Model\n",
        "class DOATA(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # Load ViT backbone with correct configuration\n",
        "        self.backbone = ViTModel.from_pretrained(\n",
        "            \"google/vit-base-patch16-224-in21k\",\n",
        "            add_pooling_layer=False,\n",
        "            ignore_mismatched_sizes=True\n",
        "        )\n",
        "\n",
        "        # Freeze backbone initially\n",
        "        for param in self.backbone.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.occlusion_estimator = OcclusionEstimator(config)\n",
        "        self.temporal_aggregator = TemporalAggregator(config)\n",
        "\n",
        "        # Classifier with proper initialization\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(config.embed_dim),\n",
        "            nn.Linear(config.embed_dim, 512),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(512, 11)  # 11 classes for UCF11\n",
        "        )\n",
        "\n",
        "        # Initialize weights correctly\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for name, module in self.named_modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                nn.init.xavier_uniform_(module.weight)\n",
        "                if module.bias is not None:  # Fixed typo: was 'biases'\n",
        "                    nn.init.constant_(module.bias, 0)\n",
        "            elif isinstance(module, nn.LayerNorm):\n",
        "                nn.init.constant_(module.weight, 1.0)\n",
        "                nn.init.constant_(module.bias, 0)  # Fixed typo: was 'biases'\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C, H, W = x.shape\n",
        "        x = x.view(B*T, C, H, W)\n",
        "\n",
        "        # Get features from backbone\n",
        "        with torch.no_grad():\n",
        "            features = self.backbone(x).last_hidden_state[:, 1:]  # Remove CLS token\n",
        "\n",
        "        features = features.view(B, T, -1, self.config.embed_dim)\n",
        "        occlusion_maps = self.occlusion_estimator(features)\n",
        "        temporal_features = self.temporal_aggregator(features, occlusion_maps)\n",
        "        logits = self.classifier(temporal_features.mean(dim=1))\n",
        "\n",
        "        return logits, occlusion_maps\n",
        "\n",
        "\n",
        "# Training Utilities\n",
        "def get_ucf11_splits(root_dir, split='train'):\n",
        "    \"\"\"Load UCF11 dataset with nested subdirectory structure\"\"\"\n",
        "    base_dir = os.path.join(root_dir, 'UCF11_updated_mpg')\n",
        "    if not os.path.exists(base_dir):\n",
        "        raise FileNotFoundError(f\"UCF11 directory not found at {base_dir}\")\n",
        "\n",
        "    # Get all class directories\n",
        "    classes = sorted([d for d in os.listdir(base_dir)\n",
        "                     if os.path.isdir(os.path.join(base_dir, d))])\n",
        "\n",
        "    if not classes:\n",
        "        raise FileNotFoundError(f\"No classes found in {base_dir}\")\n",
        "\n",
        "    class_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n",
        "    file_list = []\n",
        "    video_extensions = ('.mpg', '.avi', '.mp4', '.mpeg')\n",
        "\n",
        "    for class_name in classes:\n",
        "        class_dir = os.path.join(base_dir, class_name)\n",
        "\n",
        "        # Recursively find all video files in subdirectories\n",
        "        videos = []\n",
        "        for root, _, files in os.walk(class_dir):\n",
        "            for file in files:\n",
        "                if file.lower().endswith(video_extensions):\n",
        "                    videos.append(os.path.join(root, file))\n",
        "\n",
        "        if not videos:\n",
        "            warnings.warn(f\"No videos found in class: {class_name}\")\n",
        "            continue\n",
        "\n",
        "        # Sort videos for consistent train/test split\n",
        "        videos = sorted(videos)\n",
        "        split_idx = int(0.8 * len(videos))  # 80/20 split\n",
        "\n",
        "        if split == 'train':\n",
        "            selected_videos = videos[:split_idx]\n",
        "        else:\n",
        "            selected_videos = videos[split_idx:]\n",
        "\n",
        "        for video_path in selected_videos:\n",
        "            file_list.append((video_path, class_to_idx[class_name]))\n",
        "\n",
        "    if not file_list:\n",
        "        raise RuntimeError(f\"No videos found for {split} split. Check dataset structure.\")\n",
        "\n",
        "    print(f\"Found {len(file_list)} videos for {split} split\")\n",
        "    return file_list\n",
        "\n",
        "# Updated training function with Hugging Face token handling\n",
        "def train():\n",
        "    config = Config()\n",
        "\n",
        "    # Handle Hugging Face token\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "    except:\n",
        "        HF_TOKEN = None\n",
        "        warnings.warn(\"No HF_TOKEN found, using anonymous access\")\n",
        "\n",
        "    # Initialize with deterministic behavior\n",
        "    torch.manual_seed(42)\n",
        "    if config.device == 'cuda':\n",
        "        torch.cuda.manual_seed_all(42)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    try:\n",
        "        # Load datasets\n",
        "        train_files = get_ucf11_splits(config.root_dir, 'train')\n",
        "        test_files = get_ucf11_splits(config.root_dir, 'test')\n",
        "\n",
        "        train_dataset = UCF11Dataset(train_files, config=config)\n",
        "        test_dataset = UCF11Dataset(test_files, config=config)\n",
        "\n",
        "        # Filter out invalid samples\n",
        "        train_dataset.file_list = [f for i, f in enumerate(train_dataset.file_list)\n",
        "                                if train_dataset[i]['label'] != -1]\n",
        "        test_dataset.file_list = [f for i, f in enumerate(test_dataset.file_list)\n",
        "                               if test_dataset[i]['label'] != -1]\n",
        "\n",
        "        print(f\"\\nTraining samples: {len(train_dataset)}\")\n",
        "        print(f\"Test samples: {len(test_dataset)}\\n\")\n",
        "\n",
        "        # DataLoaders\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=config.batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=min(2, os.cpu_count()-1),\n",
        "            pin_memory=True,\n",
        "            persistent_workers=True\n",
        "        )\n",
        "\n",
        "        test_loader = DataLoader(\n",
        "            test_dataset,\n",
        "            batch_size=config.batch_size,\n",
        "            num_workers=min(2, os.cpu_count()-1),\n",
        "            pin_memory=True,\n",
        "            persistent_workers=True\n",
        "        )\n",
        "\n",
        "        # Initialize model\n",
        "        model = DOATA(config).to(config.device)\n",
        "\n",
        "        # Optimizer\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            model.parameters(),\n",
        "            lr=config.lr,\n",
        "            weight_decay=0.01\n",
        "        )\n",
        "\n",
        "        # Training loop\n",
        "        best_acc = 0.0\n",
        "        for epoch in range(config.epochs):\n",
        "            model.train()\n",
        "            train_loss = 0.0\n",
        "            correct = 0\n",
        "            total = 0\n",
        "\n",
        "            for batch_idx, batch in enumerate(train_loader):\n",
        "                videos = batch['video'].to(config.device, non_blocking=True)\n",
        "                labels = batch['label'].to(config.device, non_blocking=True)\n",
        "\n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "                logits, _ = model(videos)\n",
        "                loss = F.cross_entropy(logits, labels)\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "\n",
        "                train_loss += loss.item()\n",
        "                _, predicted = logits.max(1)\n",
        "                total += labels.size(0)\n",
        "                correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "                if batch_idx % 20 == 0:\n",
        "                    print(f\"Epoch {epoch+1}/{config.epochs} | \"\n",
        "                          f\"Batch {batch_idx}/{len(train_loader)} | \"\n",
        "                          f\"Loss: {loss.item():.4f} | \"\n",
        "                          f\"Acc: {100.*correct/total:.2f}%\")\n",
        "\n",
        "            # Validation\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "            val_correct = 0\n",
        "            val_total = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch in test_loader:\n",
        "                    videos = batch['video'].to(config.device, non_blocking=True)\n",
        "                    labels = batch['label'].to(config.device, non_blocking=True)\n",
        "\n",
        "                    logits, _ = model(videos)\n",
        "                    loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "                    val_loss += loss.item()\n",
        "                    _, predicted = logits.max(1)\n",
        "                    val_total += labels.size(0)\n",
        "                    val_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "            # Print epoch summary\n",
        "            train_acc = 100. * correct / total\n",
        "            val_acc = 100. * val_correct / val_total\n",
        "            print(f\"\\nEpoch {epoch+1} Summary:\")\n",
        "            print(f\"Train Loss: {train_loss/len(train_loader):.4f} | Acc: {train_acc:.2f}%\")\n",
        "            print(f\"Val Loss: {val_loss/len(test_loader):.4f} | Acc: {val_acc:.2f}%\")\n",
        "\n",
        "            # Save best model\n",
        "            if val_acc > best_acc:\n",
        "                best_acc = val_acc\n",
        "                torch.save({\n",
        "                    'epoch': epoch+1,\n",
        "                    'state_dict': model.state_dict(),\n",
        "                    'best_acc': best_acc,\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                }, 'best_model.pth')\n",
        "                print(f\"Saved best model with val acc: {best_acc:.2f}%\")\n",
        "\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "        print(f\"\\nTraining complete! Best validation accuracy: {best_acc:.2f}%\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError during training: {str(e)}\")\n",
        "        print(\"\\nDebugging steps:\")\n",
        "        print(\"1. Try running with num_workers=0\")\n",
        "        print(\"2. Verify your HF_TOKEN is set if using private models\")\n",
        "        print(\"3. Check CUDA memory with nvidia-smi\")\n",
        "        print(\"4. Test with smaller batch size\")\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train()\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install av"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvjN1QSeQoVo",
        "outputId": "73fe96ef-463f-454e-a530-6b4755ea54f7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting av\n",
            "  Downloading av-14.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
            "Downloading av-14.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.2/35.2 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: av\n",
            "Successfully installed av-14.3.0\n"
          ]
        }
      ]
    }
  ]
}